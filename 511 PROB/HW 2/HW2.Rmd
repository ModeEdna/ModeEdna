---
title: "Hw2"
author: "Eduardo Armenta"
date: "2022-10-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# HW 2

## Problem 1
Consider the random variable defined by counting the number of failures until the first success, for inde- pendent trials with success probability p. Given p between 0 and 1, the **R** commands **myattempts(p)** and **rgeom(1,p)** both simulate this random variable. Find a way of demonstrating that the two commands indeed give the same results, for five different values of p. You may find the table() function useful.

Choose five p’s of your choice, e.g. p ∈ {.1, .3, .5, .7, .9}. Run myattempts and rgeom each 10000 times.

**a**. For the first p, store the fraction of outcomes in the simulation in the columns of a suitable data frame
and compare (Hint: use 3 columns to compare rgeom(), myattempts() and dgeom() ) (10 points)
```{r}
# libraries
library(tidyverse)

# copying mytoss function
mytoss = function(p){
  u <- runif(1)
  x <- as.numeric(u < p)
  return(x)
}
myattempts = function(p){ counter <- 0
while (mytoss(p) == 0){ counter <- counter + 1 }
return(counter) }

# 1.1 how are we supposed to use rgeom() here?
sims <- 10000
p = .75
dfd1 <- data.frame(table(rgeom(sims,p)))[1:5,]
dfd2 <- data.frame(table(replicate(sims,myattempts(p))))[1:5,]
df <- data.frame(dfd1,dfd2)
df <- subset(df, select = -c(Var1.1))
names(df)[1] <- 'Unit'
names(df)[2] <- 'rgeom'
names(df)[3] <- 'myattempts'
df$rgeomFraction <- df$rgeom/10000
df$myattemptsFraction <- df$myattempts/10000
vector <- c(0,1,2,3,4)
df$dgeom <- dgeom(vector,.75)
df
```

### Comparison
The table shows that the probabilites of each distribution almost match up with the real probability.

**b**. For the second p, Compare distribution by computing statistics such as mean and standard deviation.
(5 points)
```{r}
sims <- 10000
cols = 3
p = .3
df2 <- data.frame(matrix(NA,nrow=sims,ncol=cols))
for (i in 1:sims){
  df2[i,1] <- rgeom(1,p)
  df2[i,2] <- myattempts(p)
}
# sds of each simulation
sd(df2$X1)
sd(df2$X2)
# means for each simulation
mean(df2$X1)
mean(df2$X2)
# variance for each simulation
var(df2$X1)
var(df2$X2)
```

### Comparison
Same as before, the statistics show basically the same distributions.

**c**. For the third p, plot both distributions as histograms in the same plot. (5 points)
```{r}
sims <- 10000
cols = 3
p = .5
df3 <- data.frame(matrix(NA,nrow=sims,ncol=cols))
for (i in 1:sims){
  df3[i,1] <- rgeom(1,p)
  df3[i,2] <- myattempts(p)
}
c1 <- rgb(0,0,255,max = 255, alpha = 250, names = "blue")
c2 <- rgb(255,0,0, max = 255, alpha = 150, names = "red")
hga <- hist(df3$X1, plot = FALSE)
hgb <- hist(df3$X2, plot = FALSE)
plot(hga, col=c1, main = "Histogram of both functions' dsitribution", xlab='Number of attempts until first success (0.5)')
plot(hgb, col=c2, add=TRUE)
```

### Comparison
It's barely visible, but the histograms are stacked on top of each other, which means that the distribution is the same.

**d**. For the fourth p, make side-by-side box plots. (5 points)
```{r}
sims <- 10000
cols = 3
p = .7
df4 <- data.frame(matrix(NA,nrow=sims,ncol=cols))
for (i in 1:sims){
  df4[i,1] <- rgeom(1,p)
  df4[i,2] <- myattempts(p)
}
library(reshape)
library(ggplot2)
df4.1 <- melt(df4)
df4.1$value <- df4.1$value
ggplot(df4.1) + geom_boxplot(aes(x=variable, y=value, color=variable)) + ylim(-1, 10)
```

### Comaparison
The box plots again show the same distribution. Even the outliers are of the same magnitude.

**e**. For the fifth p, plot the two empirical distribution functions in the same plot. (5 points)
```{r}
sims <- 10000
cols = 3
p = .2
df5 <- data.frame(matrix(NA,nrow=sims,ncol=cols))
for (i in 1:sims){
  df5[i,1] <- rgeom(1,p)
  df5[i,2] <- myattempts(p)
}
df5.1 <- melt(df5)
library(ggplot2)
colors <- c( 'royalblue4', 'sienna1')
ggplot(df5.1, aes(x = value, color = variable)) + scale_color_manual(values = colors) + stat_ecdf(lwd = 1.25, geom = "line") +
  ggtitle('ECDF of both distributions') + labs(x='Failures until first success',y='Probability')
```

### Comparison
The ecdfs give the same line. They are barely distinguishable; just like the histograms from earlier.

## Problem 2
Consider the following random experiment: draw a uniformly distributed random number X1 from the interval (0, 1). Next, draw a uniformly distributed random number X2 from the interval (0, 1 + X1), a uniformly distributed random number X3 from the interval (0, 1 + X2) and so on until X20.
Use a monte carlo simulation to give an approximate answer to What is the mean value of X20? and use a histogram to identify the distribution of X20.
X1 ∼unif(0,1)X2 ∼unif(0,1+X1)X3 ∼unif(0,1+X2). . . X20 ∼unif(0,1+X19)
The R command for drawing a uniformly distributed random number from the interval (0, b) is runif(1,min
= 0, max = b).
```{r}
iters = 10000
df <- data.frame(matrix(NA,ncol=iters,nrow=20))
for (z in 1:iters){
  x <- 0
  for (i in 1:20){
    value <- runif(1, min=0, max=1+x)
    x <- value
    df[i,z] <- value
  }
}
hist(as.numeric(df[20,]))
mean(as.numeric(df[20,]))
```

### Observations
The distribution is basically exponential; it would be more apparent if our bin values are 1 units in width rather than 0.5. The mean is almost 1.

## Problem 3
Suppose that the daily power consumption of a major city, X, has a Gamma distribution with shape parameter r = 4 and scale parameter ρ = 2. Use R to compute the following quantities:

a. Prob(X ≤ 12) (3 Points)
```{r}
pgamma(q=12,shape=4,scale=2)
```

b. Prob(X > 5) (3 Points)
```{r}
pgamma(q=5,shape=4,scale=2,lower.tail = FALSE)
```

c. Prob(|X − 8|) < 1 (4.5 Points)
```{r}
1 - (pgamma(q=7,shape=4,scale=2,lower.tail = TRUE) + pgamma(q=9,shape=4,scale=2,lower.tail = FALSE))
```

d. z such that Prob(X < z) = .95 (4.5 Points)
```{r}
qgamma(p=0.95,shape=4,scale=2)
```

## Problem 4
Probability theory says that a binomial distribution, B(n,p) is close to that of a normal distribution with mean np and standard deviation 􏰀np(1 − p), if np and n(1 − p) are both sufficiently large, e.g. at least 10.
Check this by plotting both cumulative distribution functions in the same figure, using a staircase plot for the binomial distribution and a line plot for the normal distribution, for three different cases: a case where both np and n(1−p) are large, a case where np is large and n(1−p) < 10, and a case where np < 10 and n(1 − p) < 10.
Describe what happens in all three cases. In what sense are the cdf’s not close in cases 2 and 3? (Hint: Compare with a cdf of the normal distribution)
```{r}
## Case 1
s <- 10000
p <- .5
n <- 50
mean <- n*p
sd <- sqrt(n*p*(1-p))
bd <- rbinom(n=s,size=n,p=p)
nd <- rnorm(n=s,mean=mean,sd=sd)
# ensuring rules are met
n*p
n*(1-p)
# plotting
cdfb <- ecdf(bd)
#plot(cdfb)
cdfn <- ecdf(nd)
plot(cdfn,main='ECDF of both distributions, both requirements > 10')
lines(cdfb)
```

**Comments**: the cdfs are both very close. you can see one tracing the other.

```{r}
## Case 2 NP is small and n(1-p) is large
s <- 10000
p <- .9
n <- 50
mean <- n*p
sd <- sqrt(n*p*(1-p))
bd <- rbinom(n=s,size=n,p=p)
nd <- rnorm(n=s,mean=mean,sd=sd)
# ensuring rules are met
n*p
n*(1-p)
# plotting
cdfb <- ecdf(bd)
#plot(cdfb)
cdfn <- ecdf(nd)
plot(cdfn,main='ECDF of both distributions, only np > 10')
lines(cdfb)
```

**Comments**: the ecdfs have similar shapes, but we can see the steps on the graph more clearly than before, it no longer looks continuous

```{r}
## Case 3 NP and n(1-p) are both less than 10
s <- 10000
p <- .1
n <- 10
mean <- n*p
sd <- sqrt(n*p*(1-p))
bd <- rbinom(n=s,size=n,p=p)
nd <- rnorm(n=s,mean=mean,sd=sd)
# ensuring rules are met
n*p
n*(1-p)
# plotting
cdfb <- ecdf(bd)
#plot(cdfb)
cdfn <- ecdf(nd)
plot(cdfn,main='ECDF of both distributions, both requirements < 10')
lines(cdfb)
```

 **Comments**: the ecdfs here are completely different. the steps are too few to resemble a curve. we're just looking at ~7 steps here.

## Problem 5
A graphical technique for checking whether a sample has an approximate normal distribution is a “quantile- quantile” plot. The R command is qqnorm(x), where x is the vector of sample values. If the plot is approximately a straight line, then this suggests that the sample comes from a normal distribution. Use the dataset from the openintro package to find out which of the the four distributions (three exams and course grade) is the closest to a normal distribution? You can load the dataset with the following commands library(openintro) and data(exam_grades). Explore this by making qqnorm() and qqline() plots of the four distributions. How close to straight lines are the plots in each case? How do the plots differ from straight lines? Hint: Make sure to remove the NA values.
```{r}
library(openintro)
data("exam_grades")
exam_grades <- exam_grades %>% drop_na()
ex1 <- exam_grades$exam1
ex2 <- exam_grades$exam2
ex3 <- exam_grades$exam3
courseGrade <- exam_grades$course_grade

# exam grades 1
qqnorm(ex1)
qqline(ex1)
# exam grades 2
qqnorm(ex2)
qqline(ex2)
# exam grades 3
qqnorm(ex3)
qqline(ex3)
# course grade
qqnorm(courseGrade)
qqline(courseGrade)
```

### Observations
qqnorm plots show that the course grades are closest to a normal distribution, while exam 1 and 2 have the curviest graphs.


## Problem 6
If X has a continuous distribution with cumulative distribution function F, then the new random variable U = F(X) has a uniform U(0,1) distribution. Verify this with simulations for three different continuous distributions of your choice, by making a random sample of sufficient size, sorting it, plugging it into the cdf F, and plotting the result.
```{r}
# exponential
vals <- rexp(1000,1)
probs <- pexp(vals,1)
plot.ecdf(probs)
# normal
vals <- rnorm(1000)
probs <- pnorm(vals)
plot.ecdf(probs)
# Cauchy
vals <- rcauchy(1000)
probs <- pcauchy(vals)
plot.ecdf(probs)
```

### Observations
All three graphs have a straight, diagonal line on the ecdf, so we can be sure that the distributions are uniform.

## Bonus Question
Suppose for X = X1 +X2 is the sum of two exponentially distributed random variables with the same param- eter λ. Then Xα is very nearly normally distributed for a suitable choice of α. Determine an approximate value for α (within 0.05), using a simulation and qqnorm() plots for each of your choices of α.

```{r}
vals <- rgamma(1000,2,3) # sum of two exponential random variables is a gamma distribution
p <- 0.01 # probability
a <- 0.1 # alpha value
while(p <= 0.05){ # loop through values of p up to 0.05
  x <- vals^a # raise to the power of alpha
  p <- shapiro.test(x)[2] # check normality
  a <- a + 0.01 # increase alpha
}
qqnorm(x) # see normality distribution
```


