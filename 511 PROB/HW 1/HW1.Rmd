---
title: "HW1"
author: "Eduardo Armenta"
date: "2022-09-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# ANLY 511 HW1

##Problem 1 (25 points)

A. Create a standard 52 card deck, refer to the card deck created in Bootcamp Assignment 6-problem 1 (5 points)
```{r 1a}
suits <- c("Diamonds", "Clubs", "Hearts", "Spades")
numbers <- c("Ace", "Two", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", "Ten", "Jack", "Queen", "King")
(deck <- expand.grid(number=numbers, suit=suits))
# creating a second deck with values from both columns in one, in case a future problem requires this version
(deck2 <- paste(deck$number, deck$suit))
```

B. What is the probability of getting two diamonds with replacement? (5 points)
```{r 1b}
## i'm assuming what is meant by the question is the probability of getting consecutive diamonds
(13/52)*(13/52)
```

C. What is the probability of getting two diamonds without replacement? (5 points)
```{r 1c}
(13/52)*(12/51)
```

D. Assuming a 5 card poker hand, what is the probability of having four of a kind? (Hint. The probability of a 4 of a kind is equal to total number of ways to get a four of a kind divided by the total number of ways to choose 5 cards from a 52 card deck) (10 points)
```{r 1d}
library(gtools)
(aces <- paste("Ace", suits))
hands <- combinations(52, 5, v = deck2)
# i get the probability of getting 4 aces and multiply by 13, which represents all numbers possible of which to get 4 of-a-kind
(mean(hands[,1] %in% aces & hands[,2] %in% aces & hands[,3] %in% aces & hands[,4] %in% aces))*13
```

## Problem 2 (20 points)
Using the poker dataset you just created above:

A. Run four simulations that pull 10, 100, 1000, and 10000 cards respectively with replacement. Use the `table` function to find the distribution of suit counts. (5 Points)
```{r 2a}
table(sample(suits, size=10, replace=TRUE))
table(sample(suits, size=100, replace=TRUE))
table(sample(suits, size=1000, replace=TRUE))
table(sample(suits, size=10000, replace=TRUE))
```

B. What do you notice about the distribution of counts as more simulations are run? (5 Points)

The distribution gets closer and closer to even, or 25% each, as we run more simulations. If we were to run a simulation with 1M samples, it would virtually have a spread of 25% distribution per suit. 

C. Run a simulation for Problem #1 part D to answer the question. Compare the results (hint. they should be close) (10 points)
```{r 2c}
numbers2 <- c("Ace", "Two", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", "Ten", "Jack", "Queen", "King", "Ace", "Two", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", "Ten", "Jack", "Queen", "King", "Ace", "Two", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", "Ten", "Jack", "Queen", "King", "Ace", "Two", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", "Ten", "Jack", "Queen", "King")
set.seed(123)
output <- replicate(1e5, sample(numbers2, 5, replace = FALSE))
output[, 1:10]

df <- as.data.frame(output)
(sum(table(stack(df)) == 4))/100000
```
The results are basically the same. It is .00003 points off, which is a minuscule difference.

## Problem 3 (15 points)
Consider the baby names data for 2014. Write a function that computes the conditional probability P(gender=F|name=XXX) for a given character string XXX. Next, use the function to compute conditional probabilities for the 10 most common female baby names of that year (In other words what is probability the child is a female given the name is XXX?). For which female baby name of the top 10 is this conditional probability maximal? What does this mean?
```{r 3}
df <- read.csv('/Users/modeedna/Desktop/SCHOOLS/GEORGETOWN/511 PROB/HW 1/yob2014.txt', header = FALSE, col.names = c('Name','Sex', 'Count'))

prob <- function(baby_name){
  female <- df[df$Name == baby_name & df$Sex == 'F',]$Count
  male <- df[df$Name == baby_name & df$Sex == 'M',]$Count
  return(female/(female+male))
}

prob('Emma')
prob('Olivia')
prob('Sophia')
prob('Isabella')
prob('Ava')
prob('Mia')
prob('Emily')
prob('Abigail')
prob("Madison")
prob('Charlotte')

```
Emma has a the maximal conditional probability at .9994, which means that out of the top 10 names, Emma has the biggest ratio of Female to Male genders.


## Problem 4 (15 points)
Many adults find videos online and then re-post the videos on social media sights. The Pew Internet and American Life Project has found this behavior continues to increase through various polls. The results of such polling is summarized in the table below. A count for `UserCreated` means an individual from that age range prefers watching `UserCreated` videos and a count for `TV` means an individual from that age range prefers watching `TV` videos.

```{r echo = FALSE}
ViewerAge <- c("18-34","35-54","55+","ColumnTotal")
UserCreated <- c(30,10,3,43)
TV <- c(38,10,9,57)
RowTotal <- c(68,20,12,100)
as.data.frame(cbind(ViewerAge,UserCreated,TV,RowTotal))
```

Answer the following questions referring to the table above

A. Probability that a viewer is aged 18-34 (3 points)
```{r 4a}
# Total number of viewers in the age category over total people in the study
68/100
```

B. Probability that a viewer prefers watching `TV` videos (4 points)
```{r 4b}
# Total number of people who prefer TV over total people in the study
57/100
```

C. Percentage of viewers who are 18-34 and prefer watching `UserCreated` videos (4 points)
```{r 4c}
# Amount of people who fall under the category over total people
cat(((30/100)*100), '%')
```

D. Percentage of viewers who are 35-54 or prefer watching `UserCreated` videos (4 points)
```{r 4d}
# amount of people who fall under either categories minus those who fall under both over all people
cat((((43+20-10)/100)*100), '%')
```

## Problem 5 (15 points)

Suppose $X$ and $Y$ are independent random variables that both have uniform $U(0,1)$ distributions. Consider the events

$$
A = \{X < \frac{1}{2}\}, \quad B = \{Y < \cos \pi X\} \, . 
$$

Then $P(A) = \frac{1}{2}$. Use __R__ and simulations to estimate $P(B), \, P(A|B), P(B|A)$. 

a) `R` will automatically generate independent random variables in a simulation. Use `runif(n)` to simulate 100,000 Uniform(0,1) distributed random values. Make a data frame for $X$ and $Y$ uniform random variables, compute two additional columns for the events $A$ and $B$ as given in the problem, and estimate the relevant probabilities by sub-setting. (4 points)
```{r 5a}
# creating the dataframe
xr <- runif(100000, min=0, max=1)
yr <- runif(100000, min=0, max=1)

dfr <- data.frame(xr, yr)

dfr$A <- ifelse(dfr$xr < .5, TRUE, FALSE)
dfr$B <- ifelse(dfr$yr < cos(pi*xr), TRUE, FALSE)

head(dfr, 10)
```

b) Find $P(B)$ (3 points)
```{r 5b}
# Take the mean of TRUE and FALSE column for B
mean(dfr$B)
```

c) Find $P(A|B)$ (4 points)
```{r 5c}
# Take the sum of column A when B is TRUE and divide by the sum of column that B when B is TRUE
sum(dfr[dfr$B==TRUE,]$A) / sum(dfr[dfr$B==TRUE,]$B)
```

d) Find $P(B|A)$ (4 points)
```{r 5d}
# Take the sum of column B when A is TRUE and divide it by the sum of column A when A is TRUE
sum(dfr[dfr$A==TRUE,]$B) / sum(dfr[dfr$A==TRUE,]$A)
```

## Problem 6 (10 points)
Take 5 `R` functions of your choice and find/compare/contrast their parameters, output, and usability with the `python` function that accomplishes the same goal. (2 points per function comparison)

1. The most basic and used functions for either are likely the `import` and `library` functions. Although they're not the flashiest, their purpose is load the packages you need to use for your code. I would say `library` is better because you don't have to repeatedly call the package in order to use the function, like `Python`. On the other hand, having to call the package helps with tracing your work.
2. The next basic functions we use a lot in class are the ones that read in data from a csv file. That is, `pd.read_csv` and `read.csv`. They take similar parameters: path, separator, header, column names, etc. The outcome for the functions is the same, which is reading in your data.
3. Another function that I use a lot in both languages are `length()` and `len()`, which give you the length of the datatype you are passing through it. For `R`, you can only pass either an `R` object or a value. For `Python`, you can only pass in an object and it'll spit out its length. The output is similar with the difference being that `Python` can't take a value, only an object.
4. The `sample` function from `R`, and `df.sample()` function from Python take samples from a data frame. The `R` version only permits us to pass the df from which to sample and the number of samples we want. On the other hand, the `Python` version allows us to select amount to sample by number or fraction, whether we want to do it with replacement, weights for probability of selecting x row(s), and a random state parameter for selecting a seed. Although the output can look similar, `Python`, offers a more robust function to sample data frames.
5. One that is not technically a function but is widely used in `R` is the pipe operator `%>%`, but for `Python` we do `.` to tag on more functions. I like the pipe operator from `R` because it's easier to track what is being done in each step of a process and it allows you to read others' code with more ease. The pipe operator takes in data and sequentially what you've done with the data, where in `Python` it's more common to create objects as you edit them (e.g. object for subset of df, then object for that subset with changed column, etc.).
6. `random.uniform` and `runif` both generate random samples of your desired distribution. The `Python` option allows us to input a low and high range for the distribution and a size for the sample, but the `R` version allows you to input a vector of probabilities that accompanies the vector of values to weight the sample however you wish.